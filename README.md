# This is an experimental repository playing with kaggle competitions, from the very beginner level.



components:

* digit-recognizer

    A simple task of mnist writen digits classification. Solved using dnn, without data augmentation (cuz I'm lazy.), with an accuracy of 98.8%.

    I somehow wonder how the top guys in the leaderboard achieved an accuracy of 100%.

* titanic

    A beginner-level competition to predict survivals of the Titanic given some information about the person on board.

* house prices

    This seems to be a real problem.

    Got a decent result using xgboost on almost raw data.

Kaggle competitions involves lots of data cleaning, lots of feature engineering and so many non-trivial things, but I can get only very little
fulfillment and satisfaction out of them. I definitely would want to be a robotics engineer better than a machine learning scientist! 
